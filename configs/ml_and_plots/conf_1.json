{
  "comments": [
    "First the machine learning algorithms are trained on a set of simulations containing examples having as features ",
    "the details of an attack on synthetic networks, and as label its effects.",
    "The label is the fraction of nodes failed when the failure propagation stops and the cascade is over.",
    "When the algorithms are trained, they learn a rule that links the details of an attack to its effects.",
    "Once the training is complete, the algorithms can be questioned. Given the details of an attack, they will ",
    "predicts its effects.",
    "Each prediction is compared to the result of the corresponding simulation to calculate the prediction error.",
    "The set of simulation examples that an algorithm is trained on is the 'training set', while the set of ",
    "simulation examples that the algorithm is questioned on is the 'test set'. We keep them separate so that, when ",
    "we test the algorithm, we can verify it learned a proper rule, and not just memorized the results.",
    "Plot avg_abs_pred_error_tree_vs_poly_4.pdf shows the average prediction errors made by different machine ",
    "learning algorithms questioned about the effects of attacks on increasingly larger sets of nodes.",
    "From this plot, we can see the entity of the error that a single prediction has on average.",
    "Plot avg_dead_pred_comparison_4.pdf shows a line with the average simulation results, and other lines with the ",
    "average predictions of different machine learning algorithms, an uses the same data as the other plot.",
    "From this plot, we can see that the average predictions closely match the average results, but this optimistic ",
    "measure should be taken with a grain of salt.",
    "Considering both plots, we can see that the average prediction error, calculated for each single prediction is a ",
    "more valuable measure, since it tells us how correct a single prediction could be for the specific situation we ",
    "are making the prediction for.",
    "The representation of the tree learned by the 2nd machine learning algorithm is saved as a .dot file, which can ",
    "be converted into an image. We polished the .dot file, added a legend node, and used it to create the plot",
    "synth_learned_tree_3.pdf.",
    "Training done on synthetic networks."
  ],
  "datasets": [
    {
      "fpath": "/home/agostino/Documents/Simulations/test_mp_12c/merged.tsv",
      "X_col_names": [
        "p_atkd_a",
        "p_tot_atkd_betw_c_i",
        "p_tot_atkd_ts_betw_c"
      ],
      "y_col_name": "p_dead",
      "info_col_names": [
        "instance",
        "seed",
        "#atkd_a"
      ]
    },
    {
      "fpath": "/home/agostino/Documents/Simulations/test_mp_12b/merged.tsv",
      "X_col_names": [
        "p_atkd_a",
        "p_tot_atkd_betw_c_i",
        "p_tot_atkd_ts_betw_c"
      ],
      "y_col_name": "p_dead",
      "info_col_names": [
        "instance",
        "seed",
        "#atkd_a"
      ]
    }
  ],
  "model_trainings": [
    {
      "dataset_num": 0,
      "model": {
        "name": "ridgecv",
        "kwargs": {"alphas": [0.01, 0.05, 0.1, 0.5, 1.0, 2.0]}
      },
      "steps": [
        {"name": "VarianceThreshold", "kwargs": {}},
        {"name": "PolynomialFeatures", "kwargs": {"degree": 4}},
        {"name": "StandardScaler", "kwargs": {}},
        {"name": "rfecv", "kwargs": {"cv": 5, "scoring": "neg_mean_absolute_error"}}
      ]
    },
    {
      "dataset_num": 0,
      "output_dir": "/home/agostino/Documents/Simulations/test_mp_12c",
      "model": {
        "name": "DecisionTreeRegressor",
        "kwargs": {"max_depth": 4, "max_leaf_nodes": 10}
      },
      "steps": [
        {"name": "VarianceThreshold", "kwargs": {}},
        {"name": "StandardScaler", "kwargs": {}},
        {"name": "rfecv", "kwargs": {"cv": 5, "scoring": "neg_mean_absolute_error"}}
      ]
    },
    {
      "dataset_num": 0,
      "model": {
        "name": "mlpregressor",
        "kwargs": {"solver": "lbfgs", "hidden_layer_sizes": [50, 25, 25, 50], "random_state": 12}
      },
      "steps": [
        {"name": "VarianceThreshold", "kwargs": {}},
        {"name": "PolynomialFeatures", "kwargs": {"degree": 3}},
        {"name": "StandardScaler", "kwargs": {}}
      ]
    }
  ],

  "plots": [
    {
      "name": "cost_by_atk_size_many",
      "group_by_col_name": "#atkd_a",
      "ax_x_label": "Initial no. of failed power nodes",
      "ax_y_label": "Measured fraction",
      "ax_y_lim": {"bottom": 0, "top": 1.0},
      "grid_kwargs": {"linewidth": 0.5},
      "fig_fpath": "/home/agostino/draft2/avg_abs_pred_error_tree_vs_poly_4.pdf",
      "overlays": [
        {
          "dataset_num": 1,
          "model_num": 0,
          "line_kwargs": {
            "label": "Avg absolute prediction error of polynomial regression",
            "marker": "^",
            "color": "b",
            "linewidth": 1,
            "capsize": 3
          }
        },
        {
          "dataset_num": 1,
          "model_num": 1,
          "line_kwargs": {
            "label": "Avg absolute prediction error of regression tree",
            "marker": "x",
            "color": "g",
            "linewidth": 1,
            "capsize": 3
          }
        },
        {
          "dataset_num": 1,
          "model_num": 2,
          "line_kwargs": {
            "label": "Avg absolute prediction error of neural network",
            "marker": "v",
            "color": "orange",
            "linewidth": 1,
            "capsize": 3
          }
        }
      ]
    },
    {
      "name": "deaths_and_preds_by_atk_size_many",
      "group_by_col_name": "#atkd_a",
      "ax_x_label": "Initial no. of failed power nodes",
      "ax_y_label": "Average fraction of resulting dead nodes",
      "ax_y_lim": {"bottom": 0, "top": 1.05},
      "grid_kwargs": {"linewidth": 0.5},
      "fig_fpath": "/home/agostino/draft2/avg_dead_pred_comparison_4.pdf",
      "overlays": [
        {
          "dataset_num": 1,
          "line_kwargs": {
            "label": "Actual fraction from simulation",
            "marker": "o",
            "color": "r"
          }
        },
        {
          "dataset_num": 1,
          "model_num": 0,
          "line_kwargs": {
            "label": "Fraction predicted by polynomial regression",
            "marker": "^",
            "color": "b"
          }
        },
        {
          "dataset_num": 1,
          "model_num": 1,
          "line_kwargs": {
            "label": "Fraction predicted by regression tree",
            "marker": "x",
            "color": "g"
          }
        },
        {
          "dataset_num": 1,
          "model_num": 2,
          "line_kwargs": {
            "label": "Fraction predicted by neural network",
            "marker": "v",
            "color": "orange"
          }
        }
      ]
    }
  ]
}
